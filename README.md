# âš™ï¸ How to Run the RAG AI Assistant Project

This guide explains how to set up, configure, and run the **RAG AI Assistant** locally or in Docker.

---

## ğŸ§© 1. Prerequisites

Before starting, ensure you have the following installed:

- ğŸ **Python 3.11+**
- ğŸ³ **Docker & Docker Compose** (optional but recommended)
- ğŸ§  **Cerebras API key** (for accessing the LLaMA model)
- ğŸ“¦ **Git** (for cloning the repository)

---

## ğŸ“ 2. Clone the Repository

```bash
git clone url
cd folder

docker-compose up --build



# ğŸ§  RAG AI Assistant with User-wise Contextual Knowledge  
> *â€œThis assistant empowers me to instantly access my personal knowledge anytime, anywhere.â€*

---

## ğŸš€ Project Overview  

**RAG AI Assistant** is a personalized retrieval-augmented generation (RAG) system that allows users to:  
- Login securely  
- Upload documents (PDF/Text)  
- Ask natural language questions  
- Retrieve accurate, context-based answers â€” powered by **Meta LLaMA** through **Cerebras SDK**  

Each user has an **isolated vector database**, ensuring data privacy and contextual precision.  

---

## ğŸ”‘ Key Features  

- ğŸ” **User Authentication & Isolation:** Each user gets a unique ID and private vector DB  
- ğŸ“„ **Document Upload & Chunking:** Upload PDFs or text files â†’ extract â†’ chunk â†’ embed  
- ğŸ§  **Contextual AI Answers:** Combines vector search + LLM for document-aware answers  
- ğŸ—„ï¸ **Chroma Vector DB:** Efficient, scalable embedding storage per user  
- ğŸ³ **Dockerized Architecture:** Fully containerized with FastAPI + Vector DB  
- ğŸ’¬ **Transparent Responses:** Each answer includes document source references  

---

## âš™ï¸ System Workflow  

1ï¸âƒ£ User logs in â†’ gets a unique ID
2ï¸âƒ£ Upload document â†’ extract text â†’ chunk â†’ embed â†’ store in user DB
3ï¸âƒ£ Ask query â†’ generate embedding â†’ search vector DB â†’ retrieve top chunks
4ï¸âƒ£ Feed retrieved context to Meta LLaMA via Cerebras â†’ generate AI answer with sources



---

## ğŸ§© Architecture  

[ Mobile / Web Client ]
|
v
[ FastAPI Backend ]
|
v
[ User-wise Chroma DB ]
|
v
[ Cerebras LLaMA Model ]


**Components:**  
- **FastAPI** â†’ REST APIs for authentication, file upload, and chat queries  
- **Chroma DB** â†’ Stores user-specific embeddings  
- **PyPDF2** â†’ Extracts text from uploaded PDFs  
- **Sentence Transformers (MiniLM-L6-v2)** â†’ Generates embeddings for semantic search  
- **Cerebras SDK + Meta LLaMA 17B** â†’ Produces contextual answers  

---

## ğŸ³ Docker Setup  

### Dockerfile  

```dockerfile
FROM python:3.11-slim
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y build-essential libglib2.0-0 libsm6 libxrender1 libxext6 && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY app/requirements.txt .
RUN pip install --no-cache-dir --upgrade pip
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY app/ .

# Create directories
RUN mkdir -p /app/file_storage /app/chroma_db

EXPOSE 8080
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"]

## Run with Docker
# Build the image
docker build -t rag-ai-assistant .

# Run the container
docker run -d -p 8080:8080 rag-ai-assistant

